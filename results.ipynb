{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from perceiver.model.core import (\n",
    "    PerceiverDecoder,\n",
    "    PerceiverEncoder,\n",
    "    PerceiverIO\n",
    ")\n",
    "\n",
    "from perceiver.model.core.classifier import ClassificationOutputAdapter\n",
    "from perceiver.model.core.adapter import TrainableQueryProvider\n",
    "\n",
    "from perceiver.model.vision.image_classifier import ImageInputAdapter\n",
    "\n",
    "from perceiver.model.core import (\n",
    "    PerceiverDecoder,\n",
    "    PerceiverEncoder,\n",
    "    PerceiverIO\n",
    ")\n",
    "\n",
    "from perceiver.model.core.classifier import ClassificationOutputAdapter\n",
    "from perceiver.model.core.adapter import TrainableQueryProvider\n",
    "\n",
    "from perceiver.model.vision.image_classifier import ImageInputAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_INPUT = 100\n",
    "\n",
    "# Fourier-encodes pixel positions and flatten along spatial dimensions\n",
    "input_adapter = ImageInputAdapter(\n",
    "  image_shape=(D_INPUT, 1),  # M = 224 * 224\n",
    "  num_frequency_bands=32,\n",
    ")\n",
    "\n",
    "# Projects generic Perceiver decoder output to specified number of classes\n",
    "output_adapter = ClassificationOutputAdapter(\n",
    "  num_classes=D_INPUT,\n",
    "  num_output_query_channels=512,  # F\n",
    ")\n",
    "\n",
    "# Generic Perceiver encoder\n",
    "encoder = PerceiverEncoder(\n",
    "  input_adapter=input_adapter,\n",
    "  num_latents=512,  # N\n",
    "  num_latent_channels=512,  # D changed from 1028\n",
    "  num_cross_attention_qk_channels=input_adapter.num_input_channels,  # C\n",
    "  num_cross_attention_heads=1,\n",
    "  num_self_attention_heads=8,\n",
    "  num_self_attention_layers_per_block=6,\n",
    "  num_self_attention_blocks=8,\n",
    "  dropout=0.0,\n",
    ")\n",
    "\n",
    "query_provider = TrainableQueryProvider(1, 512) # very arbitrary!\n",
    "\n",
    "# Generic Perceiver decoder\n",
    "decoder = PerceiverDecoder(\n",
    "  output_adapter=output_adapter,\n",
    "  output_query_provider=query_provider,\n",
    "  num_latent_channels=512,  # D\n",
    "  num_cross_attention_heads=1,\n",
    "  dropout=0.0,\n",
    ")\n",
    "\n",
    "# Perceiver IO image classifier\n",
    "model = PerceiverIO(encoder, decoder)\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6157, 0.5910, 0.6007, 0.5955, 0.5968, 0.5980, 0.6068, 0.6080, 0.5734,\n",
       "         0.6113, 0.5702, 0.5874, 0.6072, 0.6181, 0.5977, 0.5960, 0.6029, 0.5576,\n",
       "         0.6068, 0.6041, 0.6156, 0.5869, 0.5919, 0.6102, 0.6132, 0.6204, 0.5903,\n",
       "         0.6049, 0.6166, 0.5902, 0.5967, 0.6095, 0.5949, 0.6082, 0.6093, 0.6106,\n",
       "         0.6045, 0.6058, 0.6002, 0.6391, 0.5923, 0.6201, 0.5967, 0.5936, 0.5901,\n",
       "         0.5821, 0.5806, 0.6075, 0.6086, 0.6023, 0.5873, 0.5973, 0.6057, 0.6013,\n",
       "         0.5955, 0.6010, 0.6014, 0.5868, 0.5891, 0.6072, 0.5953, 0.5867, 0.5854,\n",
       "         0.6368, 0.5820, 0.5956, 0.6158, 0.5816, 0.6143, 0.6191, 0.6014, 0.6059,\n",
       "         0.6493, 0.5758, 0.6201, 0.5838, 0.6006, 0.5910, 0.5846, 0.6065, 0.6111,\n",
       "         0.6002, 0.6041, 0.6120, 0.5992, 0.6087, 0.5828, 0.6013, 0.5825, 0.6120,\n",
       "         0.6193, 0.6349, 0.5949, 0.6042, 0.5967, 0.5985, 0.6065, 0.6062, 0.5946,\n",
       "         0.9932]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './perceiver_small_random_epoch_7batch_2500.pth'\n",
    "# vec = np.hstack((np.ones(50), np.zeros(50)))\n",
    "# vec = vec.reshape(-1, D_INPUT)\n",
    "model.load_state_dict(torch.load(path,map_location='cpu'))\n",
    "model(torch.from_numpy(np.hstack((5*np.ones(100))))[None, :, None].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(np.hstack((np.ones(50), np.zeros(50))))[None, :, None].float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'load_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21284/3603909818.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'load_state_dict'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('perceiver-io')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f924eb19180ecde0db14e682b5804ad4e24b2fe50b99adcf7a85c55a74bd824"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
